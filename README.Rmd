---
always_allow_html: true
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(haven)
library(data.table)
library(tidyverse)
library(dplyr)
train <- fread("data/insurance_train.csv")
test <- fread("data/insurance_test.csv")
```

# Executive Summary

  This data set is from Kaggle (https://www.kaggle.com/anmolkumar/health-insurance-cross-sell-prediction), which describes a health insurance company collecting customers' demographics, vehicle information, and some insurance policy. The purpose of this project is to perform two types of analysis: inference and prediction. 

  As for the inference perspective, we performed various analysis on correlation (numeric), association (categorical), interaction, and mixed models. One of the interesting insights from this data is that top 2 regions (28 and 8) and bottom 2 regions (31 and 48) shows almost the same customers' demographics. The only difference is that top 2 regions have more population (data). Another insights is that when customers do not have vehicle insurance, customers who are interested in the company's insurance are willing to pay higher annual premium than customers that have no interests. 

  As for the prediction point of view, we built random forest model and XGBoost models. We obtained 67.81% for accuracy and 94.78% for sensitivity. That result demonstrates us a decent model. Surprisingly, XGBoost model does not perform as well as random forest model in terms of sensitivity score that is almost 15% lower than random forest model performed. Furthermore, I think this modeling shows us sometimes the orginal XGBoost setting works for the best. That also concludes why programmers who created XGBoost model set those initial parameters as the standard. However, XGBoost still generated AUC for 0.83. As we know higher AUC can have better performance on differentiating positive and negative classes. Therefore, we still think XGBoost model is not the worst model but not as well as random forest especially for this data set.

# Inference Analysis

## Data Exploration and Cleaning

```{r}
str(train)
summary(train)
colnames(train) # check column names for the dataset
```

```{r,echo=FALSE}
library(scales)
library(viridis)
library(hrbrthemes)
ggplot(train, aes(x=Annual_Premium, fill=Response)) + 
  geom_density(alpha = 0.3,adjust=1.5) + # Use geom_density to get density plot
  theme_ipsum() + # Set theme for plot
  theme(panel.grid.major = element_blank(), # Turn of the background grid
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  scale_x_continuous(labels=comma)+
  scale_y_continuous(labels=comma)+
  labs(x = "Annual Premium", # Set plot labels
       title = "Density plot of Annual Premium")

ggplot(train, aes(x=Age, fill=Response)) + 
  geom_density(alpha = 0.3,adjust=1.5) + # Use geom_density to get density plot
  theme_ipsum() + # Set theme for plot
  theme(panel.grid.major = element_blank(), # Turn of the background grid
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  scale_y_continuous(labels=comma)+
  labs(x = "Age", # Set plot labels
       title = "Density plot of Age")
```

```{r}
train <- train%>%
  rename(Vehicle_Insured=Previously_Insured)%>%  #change the column name to vehicle_insured 
  mutate(id=as.character(id),
         Gender=as.factor(Gender),
         Driving_License=as.factor(Driving_License),
         Vehicle_Insured=as.factor(Vehicle_Insured),
         Vehicle_Age=as.factor(Vehicle_Age),
         Vehicle_Damage=as.factor(Vehicle_Damage),
         Policy_Sales_Channel=as.factor(as.character(Policy_Sales_Channel)),
         Region_Code=as.factor(as.character(Region_Code)),
         Response=as.factor(as.character(Response)))

train$Vehicle_Age <-  fct_relevel(train$Vehicle_Age,"< 1 Year","1-2 Year","> 2 Years") #reorder level
```

## Correlation between Numeric Variables

```{r,echo=FALSE}
train_numeric <- train%>%
  dplyr::select(where(is.numeric))

cor(train_numeric)

library(corrplot)

L <- cor(train_numeric)  # make correlation matrix
corrplot(L, type="lower")  # make correlation plot
```

Not too much correlation between those three numeric variables

## Association between Categorical Variables
```{r}
chisq.test(train$Response, train$Driving_License, correct = FALSE)

chisq.test(train$Response, train$Vehicle_Damage, correct = FALSE)

chisq.test(train$Response, train$Vehicle_Insured,  correct = FALSE)

chisq.test(train$Response, train$Vehicle_Age,  correct = FALSE)
```

Since all p values are less than 0.01, we have up to 99% confidence that all four pairs of variables are associated. We can use assoc to see some directions of the association. 

```{r,fig.width=6, echo=FALSE}
train_factor <- train%>%
  dplyr::select(where(is.factor))

library(vcd)
train_factor%>%
  dplyr::select(Response,Driving_License)%>%
  assoc(shade=TRUE)

train_factor%>%
  dplyr::select(Response, Vehicle_Damage)%>%
  assoc(shade=TRUE)

train_factor%>%
  dplyr::select(Response,Vehicle_Insured)%>%
  assoc(shade=TRUE)

train_factor%>%
  dplyr::select(Response,Vehicle_Age)%>%
  assoc(shade=TRUE)
```

We can see customers without a driving license, they are way much less likely to be interested in vehicle insurance, which totally makes sense. A larger amount of customers are interested in the vehicle insurance if their car had some damages in the past. And we can see customers who does not have vehicle insurance are more likely to be interested in company's vehicle insurance. When Vehicles' ages are 1-2 year or 2 years older, customers are more likey to be interested in the vehicle insurance.

## Interaction

```{r}
intMod<- lm(Annual_Premium ~ Vehicle_Age*Gender , data = train)

summary(intMod)

library(effects)

modEffects <- effect("Vehicle_Age*Gender", intMod)

plot(modEffects)
```

We can see a huge comparison that the annual premium is much higher for `> 2 years of vehicle age` than the other vehicle ages. Female pays lower for < 1 year of vehicle age than male do, however female pays higher for >2 years and 1-2 year of vehicle age than male do.  

```{r}
intMod3<- lm(Annual_Premium ~ Response*Vehicle_Insured , data = train)

summary(intMod3)

#library(effects)

modEffects3 <- effect("Response*Vehicle_Insured", intMod3)

plot(modEffects3)
```

When customers do not have vehicle insurance, customers who are interested in the company's insurance are willing to pay higher annual premium than customers that have no interests.

## Logistic model

```{r}
logTest <- glm(Response ~ Annual_Premium +Gender+ Age + Driving_License + Vehicle_Insured + 
               Vehicle_Age + Vehicle_Damage + Region_Code+Policy_Sales_Channel, data = train, 
               family = binomial)  

exp(logTest$coefficients)
```

Male customers are about 9.5% likely to be interested in the vehicle insurance than female customers.
Customers with driving license are about 231.7% likely to be interested in the vehicle insurance.
Older vehicles(elder than 2 years) are about 331% likely to be interested in the vehicle insurance.
Damaged vehicles are about 658.7% likely to be interested in the vehicle insurance.



## Mixed Models
```{r}
library(lme4)
riMod <- lmer(Annual_Premium ~ Age+Driving_License+Vehicle_Insured+Vehicle_Age+Vehicle_Damage+(1|Region_Code), 
              data = train)

summary(riMod)
ranef(riMod)
MuMIn::r.squaredGLMM(riMod)
```
Our model accounts for about 19% (0.2010-0.0084) of the variation in region alone.

We can also use `lmerTest::lmer` to show the p values associated with the riMod model.
```{r}
riModP <- lmerTest::lmer(Annual_Premium ~ Age+Driving_License+Vehicle_Insured+Vehicle_Age+Vehicle_Damage+(1|Region_Code), 
              data = train) 

summary(riModP)
```

```{r}
library(sjPlot)

plot_model(riMod, type = "re") + 
  theme_minimal()
```

```{r}
library(merTools)

plotREsim(REsim(riMod), labs = TRUE)
```

From the two graphs above, we can see region 28 and 8's annual premium are significantly above average among all the regions. We can extract region top 2 and bottom 2 data from the train data to do further analysis. 

```{r}
region28 <- train[Region_Code=="28",]  #extract region 28

region8 <- train[Region_Code=="8",]  #extract region 8

region31 <- train[Region_Code=="31",] # extract region 31

region48 <- train[Region_Code=="48",]  #extract region 48
```

```{r,echo=FALSE}
library(scales)
top_2_region <- rbind(region28,region8)
bottom_2_region <- rbind(region31,region48)

ggplot(data=top_2_region,aes(x=Annual_Premium,fill=Region_Code))+
  geom_histogram(alpha=0.5)+
  scale_x_continuous(labels = comma,lim=c(0,100000))+
  scale_y_continuous(labels = comma)+
  theme_minimal()
  
ggplot(data=top_2_region,aes(x=Age,fill=Region_Code))+
  geom_histogram(alpha=0.5)+
  theme_minimal()

ggplot(data=bottom_2_region,aes(x=Annual_Premium,fill=Region_Code))+
  geom_histogram(alpha=0.5)+
  scale_x_continuous(labels = comma,lim=c(2500,2700))+
  scale_y_continuous(labels = comma)+
  theme_minimal()
  
ggplot(data=bottom_2_region,aes(x=Age,fill=Region_Code))+
  geom_histogram(alpha=0.5)+
  theme_minimal()
```

Top 2 regions: 
After we limit annual premium from 0 to 100K, we can see that top 2 regions cover almost the same range from 25K to 75K for the majority group of the annual premium. The age histogram shows almost the same pattern that the peaks are similar shown in around 26, 48, 53, 67 and 72 years old in both 28 and 8 regions. Thus, region 28 and 8 are not so different from each other.

Bottom 2 regions:
And it is surprised to see the bottom 2 regions also have similarities in annual premium and age too.
There is only one price for annual premium for both regions. We can see more close match in age histogram.  

# Prediction Analysis (Machine Learning)

We will use random forest and xgboost to build model respectively and then we can determine which model is the best fit for the data, but we need to create some dummy variables for some factor variables.

## Data Cleaning (Machine Learning purpose)

```{r}
library(fastDummies)

train <- train%>%
  mutate(Vehicle_Age=ifelse(Vehicle_Age=="< 1 Year","1",
                            ifelse(Vehicle_Age=="1-2 Year","2","3")))
#mark "< 1 Year" as 1, "1-2 Year" as 2, "> 2 Years" as 3
                            
train_dum <- dummy_cols(train[,c("Gender","Driving_License","Vehicle_Insured","Vehicle_Age","Vehicle_Damage")]) #make dummy variables 

train_dum <- train_dum%>%
  dplyr::select(-c(1:5)) #get rid of some columns

train_data <- cbind(train[,c("Age","Annual_Premium","Vintage")],train_dum,train[,"Response"])
# combine dummy data and normal data together
```

```{r}
train_data <- train_data%>% #adjust response(dependent) variable into human friendly text
  mutate(Response=ifelse(Response=="0","NOInterest","Interest"))%>%
  mutate(Response=as.factor(Response))

summary(train_data$Response) # we have a very imbalanced data set, around 14% of the customers are interested, around 86% of the customers are NOT interested. we can use SMOTE to deal with that later

set.seed(12345)
# for this project we only use 50000 subset to build the model
train_subset <- train_data[1:50000,]  # this train_subset would be imbalanced as well

test_subset <- train_data[50001:70000,] # we use the other 20000 subset for validation set 
```

## Random Forest
```{r}
library(randomForest)
library(caret)

library(DMwR) # load the SMOTE dealing with imbalanced data

smote_data <- SMOTE(Response ~ ., # Set prediction formula
train_subset, # Set dataset
perc.over = 100) # Select oversampling for minority class
summary(smote_data$Response) 

randomforest_mod <- randomForest(Response ~ ., # Set tree formula
                       data = smote_data, # Set dataset
                       ntree = 1000,
                       ) # Set number of trees to use
randomforest_mod
```

Lets look at the out-of-bag error for the model over the 1000 trees:
```{r}
oob_error <- randomforest_mod$err.rate[,1] # Extract oob error
plot_dat <- cbind.data.frame(rep(1:length(oob_error)), oob_error) # Create plot data
names(plot_dat) <- c("trees", "oob_error") # Name plot data

# Plot oob error
g_1 <- ggplot(plot_dat, aes(x = trees, y = oob_error)) + # Set x as trees and y as error
  geom_point(alpha = 0.3, color = "blue") + # Select geom point
  theme_bw() + # Set theme
  geom_smooth() + # Add smoothing line
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate")  # Set labels
g_1 # Print plot
```

The optimal number of trees are from 700 to 800 trees from the graph above, we will use 800 tress for tuning the model.

```{r}
set.seed(111111)
randomforest_preds <- predict(randomforest_mod, test_subset, type = "prob") # Create predictions for random forest model

# Convert predictions to classes, using 0.5
randomforest_pred_class <- rep("NOInterest", nrow(randomforest_preds))
randomforest_pred_class[randomforest_preds[,1] >= 0.5] <- "Interest"

confusionMatrix(as.factor(randomforest_pred_class), 
                test_subset$Response, positive = "Interest") 
```

For our original random forest model we have 68.32% accuracy and 94.09% sensitivity. We hope we can enhance accuracy rate and sensitivity rate by tuning the model. 

This model predicts 68.32% accuracy for all correct predictions including true positive and true negative and predicts 94.09% sensitivity for actual interested customers are correctly predicted as interested

Note that for this problem we are particularly interested in predicting the positive class corresponding to INTEREST customers, so we will look at sensitivity as well as overall accuracy.

### Tuning the randomforest model

```{r}
# start parallel processing to make machine learning much more efficiently (reduce running time)
library(doParallel)
library(foreach)
cl <- makeCluster(detectCores() - 1) # save one core only
registerDoParallel(cl)

library(randomForest)
library(caret)
mtry_vals <- c(2, 4, 5, 7, 9, 11, 14) # last parameter is 14 because we only have 14 independent variables
nodesize_vals <- c(1, 10, 15, 50, 100, 150, 200, 500, 1000)

tuning_params <- expand.grid(mtry_vals,nodesize_vals) # make a table for two parameters 
names(tuning_params) <- c("mtry_vals", "nodesize_vals") # set up column names for the table

foreach_rf <- foreach(i = 1:nrow(tuning_params), 
              .packages='randomForest',
              .combine = 'rbind') %dopar%
  {
    rf_Mod <- randomForest(Response ~ ., # Set tree formula
                          data= smote_data, # Set dataset
                          mtry = tuning_params$mtry_vals[i],
                          nodesize = tuning_params$nodesize_vals[i],
                          ntree = 800)
    
    rf_preds <- rf_Mod$predicted # Create predictions for bagging model
    
    t <- table(rf_preds,   smote_data$Response) # Create table
    c <- caret::confusionMatrix(t, positive = "Interest") # Produce confusion matrix
  
    data.frame(acc_vec=c$overall[1], # loop the accuracy results into acc_vec
               sens_vec=c$byClass[1], # loop the sensitivity results into sens_vec
               spec_vec=c$byClass[2], # loop the specificity results into sens_vec
               mtry=tuning_params[i,"mtry_vals"],
               nodesize=tuning_params[i,"nodesize_vals"])
    
  }
```

```{r Visualise Tuning,echo=FALSE}
res_db <- foreach_rf
res_db$mtry <- as.factor(res_db$mtry) # Convert tree number to factor for plotting
res_db$nodesize <- as.factor(res_db$nodesize) # Convert node size to factor for plotting
g_1 <- ggplot(res_db, aes(y = mtry, x = nodesize, fill = acc_vec)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$acc_vec), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Node Size", y = "mtry", fill = "OOB Accuracy") # Set labels
g_1 # Generate plot


g_2 <- ggplot(res_db, aes(y = mtry, x = nodesize, fill = sens_vec)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$sens_vec), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Node Size", y = "Mtry", fill = "OOB Sensitivity") # Set labels
g_2 # Generate plot


g_3 <- ggplot(res_db, aes(y = mtry, x = nodesize, fill = spec_vec)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$spec_vec), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Node Size", y = "Mtry", fill = "OOB Specificity")

g_3 # Generate plot
```

Since we cannot use one set of parameter to satisfy both accuracy and sensitivity, however 
we want to predict the positive class corresponding to INTEREST customers, so we will focus on the highest sensitivity score on second graph (g_2). It looks like the best set of parameters for this tree are mtry 2 and node size 1000.

```{r}
set.seed(111111)
rf_mod_final <- randomForest(Response ~ ., # Set tree formula
                       data = smote_data[,1:15], # Set dataset
                       ntree = 800,
                       nodesize=1000,
                       mtry=2
                       ) # Set number of trees to use
rf_mod_final # final random forest model

rf_preds_final <- predict(rf_mod_final, test_subset, type = "prob") # Create predictions for random forest model

# Convert predictions to classes, using 0.5 rule
rf_pred_class <- rep("NOInterest", nrow(rf_preds_final))
rf_pred_class[rf_preds_final[,1] >= 0.5] <- "Interest"

confusionMatrix(as.factor(rf_pred_class), 
                test_subset$Response, positive = "Interest") 
```

From the confusion matrix, we got better sensitivity 95.47%, which was a little bit higher than the naive random forest model. And it is reasonable that accuracy is 66.45% since we did not choose the parameter that were the best for enhancing accuracy score. Let's keep those scores in mind and we will use XGBoost to build the model to see how it will go. Now, we can plot the variable importance using `varImpPlot`:

The higher mean decrease in Gini, the higher variable importance for that variable would be. Therefore, we can conclude that *whether vehicle damaged or not and whether customers already have auto insurance or do not have auto insurance* are the most important variables. Next step, we will use XGBoost to build model and see if XGBoost model can perform better than random forest model.

```{r random forest variable importance}
varImpPlot(rf_mod_final)
```
## XGBoost 

```{r}
library(xgboost) # load XGBoost
library(xgboostExplainer) # Load XGboost Explainer
library(pROC) # Load proc
library(SHAPforxgboost) # Load shap for XGBoost
```

Prepare for train and test matrix for XGBoost model
```{r}
smote_data$new_response =0
smote_data$new_response[smote_data$Response=="Interest"] <- 1
#create train matrix
dtrain_ins <- xgb.DMatrix(data = as.matrix(smote_data[, 1:14]), 
                          label =smote_data$new_response) 

# Create test matrix
test_subset$new_response =0
test_subset$new_response[test_subset$Response=="Interest"] <- 1
dtest_ins <- xgb.DMatrix(data = as.matrix(test_subset[, 1:14]), 
                         label = test_subset$new_response)

```
Note: Label: 0 means NOInterst Label: 1 means Interest

```{r Train xgboost}
set.seed(111111)
bst_initial <- xgboost(data = dtrain_ins, # Set training data
               
               nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") # Set evaluation metric to use
```

we will use `OptimalCutpoints` package and `0.5 cutpoint rule`to decide which is the best optimal points for our confusion matrix

```{r xbgoost optimalcutpoints }
library(OptimalCutpoints) # load package for setting up optimal points
boost_preds <- predict(bst_initial, dtrain_ins) # Create predictions for xgboost model (dtrain_ins)
# Join predictions and actual
pred_dat <- cbind.data.frame(boost_preds, smote_data$new_response)
names(pred_dat) <- c("predictions", "response")

# use train train to determine the best optimal cut point
oc<- optimal.cutpoints(X = "predictions",   
                       status = "response",
                       tag.healthy = 0,
                       data = pred_dat,
                       methods = "MaxEfficiency")  #MaxSpSe

boost_preds_1 <- predict(bst_initial, dtest_ins) # Create predictions for xgboost model (dtest_ins)

pred_dat <- cbind.data.frame(boost_preds_1 , test_subset$new_response)
# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds_1))  #create one column of 0 
boost_pred_class[boost_preds_1 >= oc$MaxEfficiency$Global$optimal.cutoff$cutoff[1]] <- 1 
# replace those larger than cut off point as 1

t <- table(boost_pred_class, test_subset$new_response) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```

```{r 0.5 cutpoint rule}
 boost_preds_1_ins05 <- predict(bst_initial, dtest_ins) # Create predictions for xgboost model

 pred_dat_ins05 <- cbind.data.frame(boost_preds_1_ins05, test_subset$new_response)

 boost_pred_class_ins05 <- rep(0, length(boost_preds_1_ins05))
 boost_pred_class_ins05[boost_preds_1_ins05 >= 0.5] <- 1

 t <- table(boost_pred_class_ins05, test_subset$new_response) # Create table
 confusionMatrix(t, positive = "1")

```

It seems that `0.5% cutpoint rule` performed better on sensitivity(83.29%) and specificity(71.01%). 
We hope we can enhance sensitivity rate and specificity rate by tuning the model. 

This model predicts 83.29% sensitivity for actual interested customers are correctly predicted as interested and 71.01% specificity for actual non interested customers are correctly predicted as non interested.

Note that for this problem we are particularly interested in predicting the positive class corresponding to INTEREST("1") customers, so we will look at sensitivity as well as overall accuracy.

Visualize and decide the optimal number of iterations for XGBoost
```{r}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(111111)
bst <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.1, # Set learning rate
              
               nrounds = 1500, # Set number of rounds
               early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
               verbose = 0, # stay silent not printing
               nthread = 1, # Set number of parallel threads
               print_every_n = 20, # Prints out result every 20th iteration
              
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",  # auc means area under the curve
               eval_metric = "error") # Set evaluation metric to use
```

We can plot the results with ggplot:
```{r}
ggplot(bst$evaluation_log, aes(x=iter,y=test_error_mean))+
  geom_point()+
  geom_smooth()
```

Based on the graph and results, the optimal number of iterations is 402. We will set the number of iterations to 500

## Tuning the XGBoost model

Next, we will tune `max depth values` and `min child values` 
```{r tune xgb params 1}
max_depth_vals <- c(3, 5, 7, 10, 15) # Create vector of max depth values
min_child_weight <- c(1,3,5,7, 10, 15) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
auc_vec <- error_vec <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(111111)
  bst_tune <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = cv_params$max_depth[i], # Set max depth
              min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}
```
We can now the visualize the result of tuning these parameters:

```{r Visualise Tune 1,echo=FALSE}
# Join results in dataset
res_db <- cbind.data.frame(cv_params, auc_vec, error_vec)
names(res_db)[3:4] <- c("auc", "error") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print AUC heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = auc)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$auc), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "AUC") # Set labels
g_2 # Generate plot

# print error heatmap
g_3 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$error), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Minimum Child Weight", y = "Max Depth", fill = "Error") # Set labels
g_3 # Generate plot
```

Very low max depth values (1) perform poorly as the interactions in this dataset involve multiple variables. The level of interactions is controlled by the depth of the tree, it seems that high value of interaction depth (15) works the best.

Extreme high values of min child weight and appear to perform poorly, it is better for model to focus on small differences between samples.

Here we see that we have more optimal results for both highest accuracy rate (auc_vec) and lowest error rate when setting `max depth` to 15, and a `minimum child weight` to 1.

Next lets tune `gamma`, the minimum loss reduction necessary to make a further partition in a node.

```{r gamma tuning}
gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(111111)
auc_vec <- error_vec <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = gamma_vals[i], # Set minimum loss reduction for split

              
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}
```
Lets view our results to identify the value of `gamma` to use:

```{r Gamma res}
# Join gamma to values
cbind.data.frame(gamma_vals, auc_vec, error_vec)
```

It seems that a `gamma` value of 0 gives us the highest accuracy rate (auc_vec) and lowest error rate.
Before we proceed lets re-calibrate the number of rounds to use for an optimal model here:

```{r choose xgboost tree number 2}
# Use xgb.cv to run cross-validation inside xgboost
set.seed(111111)
bst <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
             
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

From the result we see the optimal number of trees to use with our current set of parameters is 300 so we are still fitting an appropriate number of trees. We will change number of trees to 450 instead of 500.

We will now tune the `subsample` and `colsample_by_tree` parameters:

```{r tune xgb samples}
subsample <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of subsample values
colsample_by_tree <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
auc_vec <- error_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(111111)
  bst_tune <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
              colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc", # Set evaluation metric to use
              eval_metric = "error") # Set evaluation metric to use
  auc_vec[i] <- bst_tune$evaluation_log$test_auc_mean[bst_tune$best_ntreelimit]
  error_vec[i] <- bst_tune$evaluation_log$test_error_mean[bst_tune$best_ntreelimit]
  
}

```
We can now the visualize the result of tuning these parameters:

```{r visualise tuning sample params,echo=FALSE}

res_db <- cbind.data.frame(cv_params, auc_vec, error_vec)
names(res_db)[3:4] <- c("auc", "error") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_4 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = auc)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$auc), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "AUC") # Set labels
g_4 # Generate plot


g_5 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = error)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$error), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "Error") # Set labels
g_5 # Generate plot

res_db
```

It seems that the optimal values of the parameters are a subsample parameter of 0.9 and a column sample by tree parameter of 0.7. This shows us that the model performs better when more of the samples are included.
We will use `subsample parameter of 0.9` and a `column sample by tree parameter of 0.7` for our model.

The final step in our tuning process is to lower the learning rate `eta` and add more trees, lets try a couple of different eta values here:

```{r eta tuning}

# Use xgb.cv to run cross-validation inside xgboost
set.seed(111111)
bst_mod_1 <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.7, # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use


set.seed(111111)
bst_mod_2 <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth =  15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.7, # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use

set.seed(111111)
bst_mod_3 <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1 , # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree =  0.7, # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
set.seed(111111)
bst_mod_4 <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0.1, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.7, # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use

set.seed(111111)
bst_mod_5 <- xgb.cv(data = dtrain_ins, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.7, # Set number of variables to use in each tree
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

We can then plot the error rate over different learning rates:

```{r eta plots,echo=FALSE}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_error_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_error_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_error_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_error_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_error_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7


```

From this it looks like an eta value of 0.1 gives the best results for this dataset. We can now fit our final model using our tuned hyper parameters:

```{r fit final xgb model}
set.seed(111111)
bst_final <- xgboost(data = dtrain_ins, # Set training data
              eta = 0.1, # Set learning rate
              max.depth =  15, # Set max depth
              min_child_weight = 1, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.7, # Set number of variables to use in each tree
               
              nrounds = 200, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # stay silent not printing
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") # Set evaluation metric to use
```

```{r final xgb_preds optimal cutpoint}
boost_preds <- predict(bst_final, dtrain_ins) # Create predictions for XGBoost model on training data

pred_dat <- cbind.data.frame(boost_preds, smote_data$new_response)
names(pred_dat) <- c("predictions", "response")
oc<- optimal.cutpoints(X = "predictions",
                       status = "response",
                       tag.healthy = 0,
                       data = pred_dat,
                       methods = "MaxEfficiency") # MaxSpSe

boost_preds <- predict(bst_final, dtest_ins) # Create predictions for XGBoost model

pred_dat <- cbind.data.frame(boost_preds , test_subset$new_response)
# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds))
boost_pred_class[boost_preds >= oc$MaxEfficiency$Global$optimal.cutoff$cutoff[1]] <- 1


t <- table(boost_pred_class, test_subset$new_response) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```


```{r}
boost_preds_2_ins05 <- predict(bst_final, dtest_ins) # Create predictions for xgboost model

 pred_dat_ins05 <- cbind.data.frame(boost_preds_2_ins05, test_subset$new_response)

 boost_pred_class_2_ins05 <- rep(0, length(boost_preds_2_ins05))
 boost_pred_class_2_ins05[boost_preds_2_ins05 >= 0.5] <- 1

 t <- table(boost_pred_class_2_ins05, test_subset$new_response) # Create table
 confusionMatrix(t, positive = "1")

```











As you may recall that our original XGBoost model's accuracy is 77% and sensitivity is 78.88%, but now we have 75.7% and 76.92% for accuracy and sensitivity. The slight decreasing percentages on both stats show us that sometimes the orginal XGBoost setting works for the best. That also concludes why programmers who created XGBoost model set those initial parameters as the standard.  

The winner of two models will be random forest based on the nearly 15% gap between the sensitivity score.

### Compare methods

We can compare different models by plotting their auc curves. For this we will compare our first model against our final balanced model. 

```{r compare auc}
library(pROC) 
# Calculate first model ROC
roc_1 = roc(test_subset$Response, boost_preds_1) # naive model
# Calculate final model ROC
roc_2 = roc(test_subset$Response, boost_preds) # final model
# Print initial model AUC
plot.roc(roc_1, print.auc = TRUE, col = "red", print.auc.col = "red")
# Print final model AUC
plot.roc(roc_2, print.auc = TRUE, print.auc.x = 0, print.auc.y = 0.6, col ="blue", print.auc.col = "blue", add = TRUE)
```

And naive models still performs a little better than our final model. 

### Variable Importance with XGBoost - Global Interpretation 

We can extract importance measures from XGBoost:

```{r XGBoost Importance}
library(SHAPforxgboost)
source("a_insights_shap_functions.r")
shap_result <- shap.score.rank(xgb_model = bst_initial, 
                X_train =  dtrain_ins,
                shap_approx = F)
var_importance(shap_result, top_n=10)
```

We can see the top three variables impacting the response whether customers will be interested in company's vehicle insurance or not:
1. Vehicle_Insured_No
2. Vehicle_Damage_No
3. Age

We can plot SHAP graph to see much more details of those variables.

```{r}
shap_long = shap.prep(shap = shap_result, as.matrix(smote_data[,1:14]), top_n = 10)
plot.shap.summary(data_long = shap_long)
```

From the SHAP graph we can see that the customers who did have vehicle insurance will be less likely to purchase company's vehicle insurance. We can recommend company adjust advertisement strategies towards customers who did not have vehicle insurance yet. Customers have no vehicle damages will be less likely to purchase the vehicle insurance. Companies can analyze further the extent to which vehicles are damaged to adjust marketing campagin. And customers with low annual premium quote will be more likely to purchase the vehicle insurance. We cannot always use "price war" methodology to apply this insight; however, if we can use competitors' data to dig deeper and find the right price premium for customers, we are confident in increasing revenue. 

### Variable Importance with XGBoost - Local Interpretation 

```{r}
library(lime)       # ML local interpretation
library(vip)        # ML global interpretation
library(pdp)        # ML global interpretation
library(caret)      # ML model building

explainer_caret <- lime(smote_data, bst_initial, n_bins = 5)

explanation_caret <- explain(
  x = test_subset[6:9,1:14],  # choose 2 rows for local interpretation
  explainer = explainer_caret, # use explainer
  n_permutations = 500, # set the number of permutations for each explanation
  n_features = 10, # the number of features to explain
  n_labels = 1, # the number of label to explain 
  dist_fun = "manhattan",
  kernel_width = 3,
  feature_select = "lasso_path" # use optimal method to select feature
  )
```

Here are the actual data for case 1 to case 4 from row 6 to 9 in test_subset:
```{r}
test_subset[6:9,15]
```

Note: Label: 0 means NOInterst Label: 1 means Interest

```{r,fig.height=4}
plot_explanations(explanation_caret)
```
The heatmap informed us of vehicle_insured strong impact versus other variables even just in four cases. 

```{r,fig.height=8}
plot_features(explanation_caret,ncol=2)
```

Case 1: When vehicle_insured_0 is less than 0.2, our model deemed case response as 0 ("NOInterest"), which means customers who did have purchased vehicle insurance will NOT be interested in buying company's vehicle insurance. 

Case 2: Although vehicle_insured_0 is a contradicted variable to the prediction in case 2, model still predicted correctly for case 2. 

Case 3 & 4: We can see that when vehicle_insured_0 is larger than 0.8, our model deemed case response as 1 ("Interest"), which means customers who did not purchase vehicle insurance before will be interested for case three and four; nevertheless, the model's result is contrary to the actual response even with the supportive variable. 

